import os
import warnings
import numpy as np
import pandas as pd
from tqdm import tqdm
from PIL import Image

import torch
from torchvision import transforms
from torchvision.models.feature_extraction import create_feature_extractor

# ----------------------- åŸºæœ¬é…ç½® -----------------------
warnings.filterwarnings("ignore")

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print('å½“å‰è®¾å¤‡:', device)

# æ¨¡å‹è·¯å¾„å’Œé¢„æµ‹è¡¨è·¯å¾„ï¼ˆå¯ä¿®æ”¹ï¼‰
model_path = 'E:/Image classification/3- Model training/model/best-0.868.pt'
result_csv_path = 'E:/Image classification/5- Model evaluation/output/æµ‹è¯•é›†é¢„æµ‹ç»“æœ.csv'
save_npy_path = 'E:/Image classification/5- Model evaluation/output/æµ‹è¯•é›†è¯­ä¹‰ç‰¹å¾.npy'

# å›¾åƒé¢„å¤„ç†
test_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# ----------------------- åŠ è½½æ¨¡å‹ -----------------------
model = torch.load(model_path, map_location=device)
model = model.eval().to(device)

# æå– avgpool ä¸­é—´å±‚ä½œä¸ºè¯­ä¹‰ç‰¹å¾
model_trunc = create_feature_extractor(model, return_nodes={'avgpool': 'semantic_feature'})
print('æ¨¡å‹åŠ è½½å®Œæˆï¼Œå·²æˆªæ–­è‡³ avgpool å±‚')

# ----------------------- å•å›¾ç‰¹å¾æå–å‡½æ•° -----------------------
def extract_feature(img_path):
    try:
        with Image.open(img_path).convert('RGB') as img_pil:
            input_tensor = test_transform(img_pil).unsqueeze(0).to(device)
            with torch.no_grad():
                features = model_trunc(input_tensor)['semantic_feature']
            return features.squeeze().detach().cpu().numpy()
    except Exception as e:
        print(f"è¯»å–å¤±è´¥: {img_path} | é”™è¯¯: {e}")
        return None

# ----------------------- æ‰¹é‡è®¡ç®—è¯­ä¹‰ç‰¹å¾ -----------------------
df = pd.read_csv(result_csv_path)
print(f'å…±æœ‰ {len(df)} å¼ å›¾åƒå¾…å¤„ç†')

semantic_features = []
valid_img_paths = []

for img_path in tqdm(df['å›¾åƒè·¯å¾„'], desc='ğŸš€ æå–è¯­ä¹‰ç‰¹å¾ä¸­'):
    feature = extract_feature(img_path)
    if feature is not None:
        semantic_features.append(feature)
        valid_img_paths.append(img_path)

semantic_array = np.array(semantic_features)
print('ç‰¹å¾æå–å®Œæˆï¼Œshape:', semantic_array.shape)

# ----------------------- ä¿å­˜ç»“æœ -----------------------
os.makedirs(os.path.dirname(save_npy_path), exist_ok=True)
np.save(save_npy_path, semantic_array)
print(f'ç‰¹å¾å·²ä¿å­˜è‡³: {save_npy_path}')
